{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a9ef925",
   "metadata": {},
   "source": [
    "# IC Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "120f8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\"\n",
    "import argparse\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pprint\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "from matdeeplearn import models, process, training\n",
    "\n",
    "config_path = 'config.yml'\n",
    "#os.path.exists(config_path)\n",
    "# os\n",
    "os.path.abspath(os.getcwd())\n",
    "\n",
    "assert os.path.exists(config_path), (\n",
    "    \"Config file not found in \" + config_path\n",
    "  )\n",
    "with open(config_path, \"r\") as ymlfile:\n",
    "    config = yaml.load(ymlfile, Loader=yaml.FullLoader)\n",
    "config[\"Job\"] = config[\"Job\"]['Inductive_Conformal']\n",
    "config[\"Models\"] = config[\"Models\"].get(\"MPNN_demo\")\n",
    "world_size = torch.cuda.device_count()\n",
    "print(world_size)\n",
    "config[\"Processing\"][\"data_path\"] = \"data/pt_data/pt_data_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed075d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Training']['train_ratio'] = 0.7\n",
    "config['Training']['val_ratio'] = 0.1\n",
    "config['Training']['test_ratio'] = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66515922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Job': {'job_name': 'my_conformal_job',\n",
       "  'error_model_path': 'error_model.pth',\n",
       "  'reprocess': 'True',\n",
       "  'model': 'MEGNet_demo',\n",
       "  'load_model': 'False',\n",
       "  'save_model': 'True',\n",
       "  'model_path': 'my_model.pth',\n",
       "  'write_output': 'True',\n",
       "  'parallel': 'True',\n",
       "  'seed': 98},\n",
       " 'Processing': {'dataset_type': 'inmemory',\n",
       "  'data_path': 'data/pt_data/pt_data_2',\n",
       "  'target_path_errors': 'error_targets.csv',\n",
       "  'target_path': 'targets.csv',\n",
       "  'dictionary_source': 'default',\n",
       "  'dictionary_path': 'atom_dict.json',\n",
       "  'data_format': 'json',\n",
       "  'verbose': 'True',\n",
       "  'graph_max_radius': 8.0,\n",
       "  'graph_max_neighbors': 12,\n",
       "  'voronoi': 'False',\n",
       "  'edge_features': 'True',\n",
       "  'graph_edge_length': 50,\n",
       "  'SM_descriptor': 'False',\n",
       "  'SOAP_descriptor': 'False',\n",
       "  'SOAP_rcut': 8.0,\n",
       "  'SOAP_nmax': 6,\n",
       "  'SOAP_lmax': 4,\n",
       "  'SOAP_sigma': 0.3},\n",
       " 'Training': {'target_index': 0,\n",
       "  'loss': 'l1_loss',\n",
       "  'train_ratio': 0.7,\n",
       "  'val_ratio': 0.1,\n",
       "  'test_ratio': 0.2,\n",
       "  'verbosity': 1},\n",
       " 'Models': {'model': 'MPNN',\n",
       "  'dim1': 100,\n",
       "  'dim2': 100,\n",
       "  'dim3': 100,\n",
       "  'pre_fc_count': 1,\n",
       "  'gc_count': 4,\n",
       "  'post_fc_count': 3,\n",
       "  'pool': 'global_mean_pool',\n",
       "  'pool_order': 'early',\n",
       "  'batch_norm': 'True',\n",
       "  'batch_track_stats': 'True',\n",
       "  'act': 'relu',\n",
       "  'dropout_rate': 0.0,\n",
       "  'epochs': 250,\n",
       "  'lr': 0.001,\n",
       "  'batch_size': 100,\n",
       "  'optimizer': 'AdamW',\n",
       "  'optimizer_args': {},\n",
       "  'scheduler': 'ReduceLROnPlateau',\n",
       "  'scheduler_args': {'mode': 'min',\n",
       "   'factor': 0.8,\n",
       "   'patience': 10,\n",
       "   'min_lr': 1e-05,\n",
       "   'threshold': 0.0002}}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4202d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "rank = 'cuda'\n",
    "print(world_size)\n",
    "data_path = config[\"Processing\"][\"data_path\"]\n",
    "job_parameters= config[\"Job\"]\n",
    "training_parameters= config[\"Training\"]\n",
    "model_parameters= config[\"Models\"]\n",
    "processing_args= config['Processing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1facff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_index': 0,\n",
       " 'loss': 'l1_loss',\n",
       " 'train_ratio': 0.7,\n",
       " 'val_ratio': 0.1,\n",
       " 'test_ratio': 0.2,\n",
       " 'verbosity': 1}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adf1475",
   "metadata": {},
   "outputs": [],
   "source": [
    "##General imports\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import copy\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import platform\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "##Torch imports\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader, Dataset\n",
    "from torch_geometric.nn import DataParallel\n",
    "import torch_geometric.transforms as T\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "##Matdeeplearn imports\n",
    "from matdeeplearn import models\n",
    "import matdeeplearn.process as process\n",
    "import matdeeplearn.training as training\n",
    "from matdeeplearn.models.utils import model_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc354a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Processing\n",
      "train length: 13860 val length: 1980 test length: 3960 unused length: 1 seed : 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varivoda/.local/lib/python3.7/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "               Layer.Parameter    Param Tensor Shape              Param #\n",
      "--------------------------------------------------------------------------\n",
      "         pre_lin_list.0.weight            [100, 114]                11400\n",
      "           pre_lin_list.0.bias                 [100]                  100\n",
      "              conv_list.0.bias                 [100]                  100\n",
      "       conv_list.0.nn.0.weight             [100, 50]                 5000\n",
      "         conv_list.0.nn.0.bias                 [100]                  100\n",
      "       conv_list.0.nn.2.weight          [10000, 100]              1000000\n",
      "         conv_list.0.nn.2.bias               [10000]                10000\n",
      "        conv_list.0.lin.weight            [100, 100]                10000\n",
      "              conv_list.1.bias                 [100]                  100\n",
      "       conv_list.1.nn.0.weight             [100, 50]                 5000\n",
      "         conv_list.1.nn.0.bias                 [100]                  100\n",
      "       conv_list.1.nn.2.weight          [10000, 100]              1000000\n",
      "         conv_list.1.nn.2.bias               [10000]                10000\n",
      "        conv_list.1.lin.weight            [100, 100]                10000\n",
      "              conv_list.2.bias                 [100]                  100\n",
      "       conv_list.2.nn.0.weight             [100, 50]                 5000\n",
      "         conv_list.2.nn.0.bias                 [100]                  100\n",
      "       conv_list.2.nn.2.weight          [10000, 100]              1000000\n",
      "         conv_list.2.nn.2.bias               [10000]                10000\n",
      "        conv_list.2.lin.weight            [100, 100]                10000\n",
      "              conv_list.3.bias                 [100]                  100\n",
      "       conv_list.3.nn.0.weight             [100, 50]                 5000\n",
      "         conv_list.3.nn.0.bias                 [100]                  100\n",
      "       conv_list.3.nn.2.weight          [10000, 100]              1000000\n",
      "         conv_list.3.nn.2.bias               [10000]                10000\n",
      "        conv_list.3.lin.weight            [100, 100]                10000\n",
      "       gru_list.0.weight_ih_l0            [300, 100]                30000\n",
      "       gru_list.0.weight_hh_l0            [300, 100]                30000\n",
      "         gru_list.0.bias_ih_l0                 [300]                  300\n",
      "         gru_list.0.bias_hh_l0                 [300]                  300\n",
      "       gru_list.1.weight_ih_l0            [300, 100]                30000\n",
      "       gru_list.1.weight_hh_l0            [300, 100]                30000\n",
      "         gru_list.1.bias_ih_l0                 [300]                  300\n",
      "         gru_list.1.bias_hh_l0                 [300]                  300\n",
      "       gru_list.2.weight_ih_l0            [300, 100]                30000\n",
      "       gru_list.2.weight_hh_l0            [300, 100]                30000\n",
      "         gru_list.2.bias_ih_l0                 [300]                  300\n",
      "         gru_list.2.bias_hh_l0                 [300]                  300\n",
      "       gru_list.3.weight_ih_l0            [300, 100]                30000\n",
      "       gru_list.3.weight_hh_l0            [300, 100]                30000\n",
      "         gru_list.3.bias_ih_l0                 [300]                  300\n",
      "         gru_list.3.bias_hh_l0                 [300]                  300\n",
      "          bn_list.0.lin.weight             [10, 100]                 1000\n",
      "         bn_list.0.norm.weight                [1000]                 1000\n",
      "           bn_list.0.norm.bias                [1000]                 1000\n",
      "          bn_list.1.lin.weight             [10, 100]                 1000\n",
      "         bn_list.1.norm.weight                [1000]                 1000\n",
      "           bn_list.1.norm.bias                [1000]                 1000\n",
      "          bn_list.2.lin.weight             [10, 100]                 1000\n",
      "         bn_list.2.norm.weight                [1000]                 1000\n",
      "           bn_list.2.norm.bias                [1000]                 1000\n",
      "          bn_list.3.lin.weight             [10, 100]                 1000\n",
      "         bn_list.3.norm.weight                [1000]                 1000\n",
      "           bn_list.3.norm.bias                [1000]                 1000\n",
      "        post_lin_list.0.weight            [100, 100]                10000\n",
      "          post_lin_list.0.bias                 [100]                  100\n",
      "        post_lin_list.1.weight            [100, 100]                10000\n",
      "          post_lin_list.1.bias                 [100]                  100\n",
      "        post_lin_list.2.weight            [100, 100]                10000\n",
      "          post_lin_list.2.bias                 [100]                  100\n",
      "                lin_out.weight              [1, 100]                  100\n",
      "                  lin_out.bias                   [1]                    1\n",
      "--------------------------------------------------------------------------\n",
      "Total params: 4397101\n",
      "Trainable params: 4397101\n",
      "Non-trainable params: 0\n"
     ]
    }
   ],
   "source": [
    "##DDP\n",
    "training.ddp_setup(rank, world_size)\n",
    "##some issues with DDP learning rate\n",
    "if rank not in (\"cpu\", \"cuda\"):\n",
    "    model_parameters[\"lr\"] = model_parameters[\"lr\"] * world_size\n",
    "\n",
    "##Get dataset\n",
    "dataset = process.get_dataset(data_path, training_parameters[\"target_index\"], True,  processing_args= config['Processing'])\n",
    "\n",
    "print('Done Processing')\n",
    "\n",
    "if rank not in (\"cpu\", \"cuda\"):\n",
    "    dist.barrier()\n",
    "\n",
    "##Set up loader\n",
    "(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    train_sampler,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    test_dataset,\n",
    ") = training.loader_setup(\n",
    "    training_parameters[\"train_ratio\"],\n",
    "    training_parameters[\"val_ratio\"],\n",
    "    training_parameters[\"test_ratio\"],\n",
    "    model_parameters[\"batch_size\"],\n",
    "    dataset,\n",
    "    rank,\n",
    "    job_parameters[\"seed\"],\n",
    "    world_size,\n",
    ")\n",
    "\n",
    "##Set up model\n",
    "model =training.model_setup(\n",
    "    rank,\n",
    "    model_parameters[\"model\"],\n",
    "    model_parameters,\n",
    "    dataset,\n",
    "    job_parameters[\"load_model\"],\n",
    "    job_parameters[\"model_path\"],\n",
    "    model_parameters.get(\"print_model\", True),\n",
    ")\n",
    "\n",
    "##Set-up optimizer & scheduler\n",
    "optimizer = getattr(torch.optim, model_parameters[\"optimizer\"])(\n",
    "    model.parameters(),\n",
    "    lr=model_parameters[\"lr\"],\n",
    "    **model_parameters[\"optimizer_args\"]\n",
    ")\n",
    "scheduler = getattr(torch.optim.lr_scheduler, model_parameters[\"scheduler\"])(\n",
    "    optimizer, **model_parameters[\"scheduler_args\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f7af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Start training\n",
    "model = training.trainer(\n",
    "    rank,\n",
    "    world_size,\n",
    "    model,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    training_parameters[\"loss\"],\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    train_sampler,\n",
    "    model_parameters[\"epochs\"],\n",
    "    training_parameters[\"verbosity\"],\n",
    "    \"my_model_temp.pth\",\n",
    ")\n",
    "\n",
    "if rank in (0, \"cpu\", \"cuda\"):\n",
    "\n",
    "    train_error = val_error = test_error = float(\"NaN\")\n",
    "\n",
    "    ##workaround to get training output in DDP mode\n",
    "    ##outputs are slightly different, could be due to dropout or batchnorm?\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=model_parameters[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    test_loader = training.DataLoader(\n",
    "                    test_dataset,\n",
    "                    batch_size=model_parameters[\"batch_size\"],\n",
    "                    shuffle=False,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=True,\n",
    "                )\n",
    "\n",
    "    val_loader = training.DataLoader(\n",
    "                    val_dataset,\n",
    "                    batch_size=model_parameters[\"batch_size\"],\n",
    "                    shuffle=False,\n",
    "                    num_workers=0,\n",
    "                    pin_memory=True,\n",
    "                )\n",
    "\n",
    "    ##Get train error in eval mode\n",
    "    train_error, train_out = training.evaluate(\n",
    "        train_loader, model, training_parameters[\"loss\"], rank, out=True\n",
    "    )\n",
    "    print(\"Train Error: {:.5f}\".format(train_error))\n",
    "\n",
    "    ##Get val error\n",
    "    if val_loader != None:\n",
    "        val_error, val_out = training.evaluate(\n",
    "            val_loader, model, training_parameters[\"loss\"], rank, out=True\n",
    "        )\n",
    "        print(\"Val Error: {:.5f}\".format(val_error))\n",
    "\n",
    "    ##Get test error\n",
    "    if test_loader != None:\n",
    "        test_error, test_out = training.evaluate(\n",
    "            test_loader, model, training_parameters[\"loss\"], rank, out=True\n",
    "        )\n",
    "        print(\"Test Error: {:.5f}\".format(test_error))\n",
    "        \n",
    "        \n",
    "if job_parameters[\"save_model\"] == \"True\":\n",
    "\n",
    "    if rank not in (\"cpu\", \"cuda\"):\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"full_model\": model,\n",
    "            },\n",
    "            job_parameters[\"model_path\"],\n",
    "        )\n",
    "    else:\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"full_model\": model,\n",
    "            },\n",
    "            job_parameters[\"model_path\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403f3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = val_error = test_error = epoch_time = float(\"NaN\")\n",
    "train_start = time.time()\n",
    "best_val_error = 1e10\n",
    "model_best = model\n",
    "##Start training over epochs loop\n",
    "train_error = train(model, optimizer, train_loader, loss, rank=rank, my_coeff = coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b79536bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "loss_all = 0\n",
    "count = 0\n",
    "for data in train_loader:\n",
    "    data = data.to(rank)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a034f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0485, 0.0465, 0.0502, 0.0466, 0.0484, 0.0464, 0.0482, 0.0450, 0.0448,\n",
       "        0.0502, 0.0466, 0.0485, 0.0448, 0.0498, 0.0484, 0.0482, 0.0447, 0.0486,\n",
       "        0.0486, 0.0464, 0.0465, 0.0465, 0.0496, 0.0485, 0.0480, 0.0463, 0.0483,\n",
       "        0.0466, 0.0482, 0.0485, 0.0449, 0.0498, 0.0449, 0.0450, 0.0464, 0.0502,\n",
       "        0.0449, 0.0465, 0.0448, 0.0498, 0.0467, 0.0463, 0.0461, 0.0487, 0.0499,\n",
       "        0.0485, 0.0482, 0.0497, 0.0484, 0.0487, 0.0482, 0.0502, 0.0463, 0.0497,\n",
       "        0.0498, 0.0472, 0.0483, 0.0500, 0.0497, 0.0485, 0.0498, 0.0483, 0.0467,\n",
       "        0.0461, 0.0500, 0.0486, 0.0502, 0.0499, 0.0464, 0.0503, 0.0452, 0.0500,\n",
       "        0.0485, 0.0466, 0.0449, 0.0447, 0.0466, 0.0499, 0.0453, 0.0451, 0.0502,\n",
       "        0.0473, 0.0496, 0.0477, 0.0465, 0.0482, 0.0496, 0.0447, 0.0487, 0.0485,\n",
       "        0.0465, 0.0464, 0.0465, 0.0465, 0.0449, 0.0499, 0.0485, 0.0464, 0.0468,\n",
       "        0.0447], device='cuda:0', grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d438f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ind = pd.DataFrame({'index':train_dataset.indices, 'set':list(np.repeat(\"train\",len(train_dataset.indices)))})\n",
    "v_ind = pd.DataFrame({'index':val_dataset.indices, 'set':list(np.repeat(\"train\",len(val_dataset.indices)))})\n",
    "tst_ind = pd.DataFrame({'index':test_dataset.indices, 'set':list(np.repeat(\"train\",len(test_dataset.indices)))})\n",
    "\n",
    "dset = pd.concat([tr_ind, v_ind, tst_ind])\n",
    "\n",
    "dset.to_csv(os.path.join(os.getcwd(),'IC_mod1_test.csv'), index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8ae6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf51531",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in train_loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1875e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_loader, 'train_loader.pth')\n",
    "torch.save(val_loader, 'val_loader.pth')\n",
    "torch.save(test_loader, 'train_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a10083",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a0c96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = pd.DataFrame(train_out, columns=['index', 'target', 'predicted'])\n",
    "target_val = pd.DataFrame(val_out, columns=['index', 'target', 'predicted']) \n",
    "target_test = pd.DataFrame(test_out, columns=['index', 'target', 'predicted']) \n",
    "target_errors = pd.concat([target_train,target_val,target_test], axis = 0)\n",
    "target_errors = target_errors.sort_values(list(target_errors), ascending=True)\n",
    "target_errors['error'] = np.absolute(target_errors['target'].apply(float) - target_errors['predicted'].apply(float))\n",
    "\n",
    "indices_list = target_errors['index'].to_list()\n",
    "my_ind = pd.read_csv(os.path.join(os.getcwd(),data_path,'targets.csv'),header = None )\n",
    "\n",
    "target_errors = target_errors.sort_values('index')\n",
    "\n",
    "target_errors['index'] = target_errors['index'].apply(int)\n",
    "indices_list = target_errors['index'].to_list()\n",
    "\n",
    "all_ind = my_ind[0].to_list()\n",
    "main_list = list(set(all_ind) - set(indices_list))\n",
    "\n",
    "#print(main_list)\n",
    "my_index = main_list[0]\n",
    "\n",
    "new_df = pd.DataFrame({\"index\":my_index,\"target\":0, 'predicted':0, \"error\":0}, index=[19801])\n",
    "target_errors = target_errors.append(new_df)\n",
    "target_errors = target_errors.reset_index(drop=True)\n",
    "target_errors = target_errors.sort_values('index')\n",
    "\n",
    "target_errors = target_errors.sort_values('index')\n",
    "target_errors = target_errors.reset_index(drop=True)\n",
    "target_errors[['index','error']].to_csv(os.path.join(os.getcwd(),data_path,'error_targets.csv'), index = False, header=False)\n",
    "\n",
    "new_data = process.get_dataset_error(data_path ,training_parameters[\"target_index\"], False, processing_args)\n",
    "\n",
    "\n",
    "error_train_subset =  torch.utils.data.Subset(new_data, train_dataset.indices)\n",
    "error_val_subset = torch.utils.data.Subset(new_data, val_dataset.indices)\n",
    "error_test_subset = torch.utils.data.Subset(new_data, test_dataset.indices)\n",
    "\n",
    "train_loader_e = training.DataLoader(\n",
    "    error_train_subset,\n",
    "    batch_size=model_parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader_e = training.DataLoader(\n",
    "                error_val_subset,\n",
    "                batch_size=model_parameters[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "test_loader_e = training.DataLoader(\n",
    "                error_test_subset,\n",
    "                batch_size=model_parameters[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f5741a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf03fb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_errors = training.model_setup(\n",
    "        rank,\n",
    "        model_parameters[\"model\"],\n",
    "        model_parameters,\n",
    "        new_data,\n",
    "        job_parameters[\"load_model\"],\n",
    "        job_parameters[\"model_path\"],\n",
    "        model_parameters.get(\"print_model\", True),\n",
    "    ) \n",
    "\n",
    "optimizer = getattr(torch.optim, model_parameters[\"optimizer\"])(\n",
    "    model_errors.parameters(),\n",
    "    lr=model_parameters[\"lr\"],\n",
    "    **model_parameters[\"optimizer_args\"]\n",
    ")\n",
    "scheduler = getattr(torch.optim.lr_scheduler, model_parameters[\"scheduler\"])(\n",
    "    optimizer, **model_parameters[\"scheduler_args\"]\n",
    ")\n",
    "\n",
    "##Start training\n",
    "model_errors = training.trainer(\n",
    "    rank,\n",
    "    world_size,\n",
    "    model_errors,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    training_parameters[\"loss\"],\n",
    "    train_loader_e,\n",
    "    val_loader_e,\n",
    "    train_sampler,\n",
    "    model_parameters[\"epochs\"],\n",
    "    training_parameters[\"verbosity\"],\n",
    "    \"my_model_error_temp.pth\",\n",
    ")\n",
    "\n",
    "train_error_e, train_out_e = training.evaluate(\n",
    "    train_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "val_error_e, val_out_e = training.evaluate(\n",
    "    val_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "test_error_e, test_out_e = training.evaluate(\n",
    "    test_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "\n",
    "target_train_e = pd.DataFrame(train_out_e, columns=['index', 'target_error', 'predicted_error'])\n",
    "target_val_e = pd.DataFrame(val_out_e, columns=['index', 'target_error', 'predicted_error']) \n",
    "target_test_e = pd.DataFrame(test_out_e, columns=['index', 'target_error', 'predicted_error']) \n",
    "target_errors_e = pd.concat([target_train_e,target_val_e,target_test_e], axis = 0)\n",
    "target_errors_e = target_errors_e.sort_values(list(target_errors_e), ascending=True)\n",
    "target_errors_e['error_2'] = np.absolute(target_errors_e['target_error'].apply(float) - target_errors_e['predicted_error'].apply(float))\n",
    "\n",
    "target_val_e_2 = copy.copy(target_val_e)\n",
    "target_val_e_2['target_error'] = target_val_e_2['target_error'].apply(float)\n",
    "target_val_e_2['predicted_error'] = target_val_e_2['predicted_error'].apply(float)\n",
    "target_val_e_2['alpha'] = np.abs(target_val_e_2['target_error']-target_val_e_2['predicted_error'])\n",
    "\n",
    "target_val_e_2 = target_val_e_2.sort_values(['alpha'], axis=0, ascending=True)\n",
    "alpha = np.percentile(target_val_e_2['alpha'], 95)\n",
    "\n",
    "target_train_e['predicted_error'] = target_train_e['predicted_error'].apply(float)\n",
    "target_train_e['lower_error_confidence_level'] = target_train_e['predicted_error'] - alpha\n",
    "target_train_e['upper_error_confidence_level'] = target_train_e['predicted_error'] + alpha\n",
    "\n",
    "target_val_e['predicted_error'] = target_val_e['predicted_error'].apply(float)\n",
    "target_val_e['lower_error_confidence_level'] = target_val_e['predicted_error'] - alpha\n",
    "target_val_e['upper_error_confidence_level'] = target_val_e['predicted_error'] + alpha\n",
    "\n",
    "target_test_e['predicted_error'] = target_test_e['predicted_error'].apply(float)\n",
    "target_test_e['lower_error_confidence_level'] = target_test_e['predicted_error'] - alpha\n",
    "target_test_e['upper_error_confidence_level'] = target_test_e['predicted_error'] + alpha\n",
    "\n",
    "\n",
    "target_train_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_train.csv'), index = False, header=True)\n",
    "target_val_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_val.csv'), index = False, header=True)\n",
    "target_test_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_test.csv'), index = False, header=True)\n",
    "\n",
    "        ##Write outputs\n",
    "if job_parameters[\"write_output\"] == \"True\":\n",
    "\n",
    "    training.write_results(\n",
    "        train_out, str(job_parameters[\"job_name\"]) + \"_train_outputs.csv\"\n",
    "    )\n",
    "    if val_loader != None:\n",
    "        training.write_results(\n",
    "            val_out, str(job_parameters[\"job_name\"]) + \"_val_outputs.csv\"\n",
    "        )\n",
    "    if test_loader != None:\n",
    "        training.write_results(\n",
    "            test_out, str(job_parameters[\"job_name\"]) + \"_test_outputs.csv\"\n",
    "        )\n",
    "\n",
    "if rank not in (\"cpu\", \"cuda\"):\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "##Write out model performance to file\n",
    "error_values = np.array((train_error.cpu(), val_error.cpu(), test_error.cpu()))\n",
    "if job_parameters.get(\"write_error\") == \"True\":\n",
    "    np.savetxt(\n",
    "        job_parameters[\"job_name\"] + \"_errorvalues.csv\",\n",
    "        error_values[np.newaxis, ...],\n",
    "        delimiter=\",\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250db8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc01037a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89d28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2183bae7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0bdbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a9142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = process.get_dataset_error(data_path ,training_parameters[\"target_index\"], False, processing_args)\n",
    "\n",
    "\n",
    "error_train_subset =  torch.utils.data.Subset(new_data, train_dataset.indices)\n",
    "error_val_subset = torch.utils.data.Subset(new_data, val_dataset.indices)\n",
    "error_test_subset = torch.utils.data.Subset(new_data, test_dataset.indices)\n",
    "\n",
    "train_loader_e = training.DataLoader(\n",
    "    error_train_subset,\n",
    "    batch_size=model_parameters[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_loader_e = training.DataLoader(\n",
    "                error_val_subset,\n",
    "                batch_size=model_parameters[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "test_loader_e = training.DataLoader(\n",
    "                error_test_subset,\n",
    "                batch_size=model_parameters[\"batch_size\"],\n",
    "                shuffle=False,\n",
    "                num_workers=0,\n",
    "                pin_memory=True,\n",
    "            )\n",
    "\n",
    "model_errors = training.model_setup(\n",
    "        rank,\n",
    "        model_parameters[\"model\"],\n",
    "        model_parameters,\n",
    "        new_data,\n",
    "        job_parameters[\"load_model\"],\n",
    "        job_parameters[\"model_path\"],\n",
    "        model_parameters.get(\"print_model\", True),\n",
    "    ) \n",
    "\n",
    "optimizer = getattr(torch.optim, model_parameters[\"optimizer\"])(\n",
    "    model_errors.parameters(),\n",
    "    lr=model_parameters[\"lr\"],\n",
    "    **model_parameters[\"optimizer_args\"]\n",
    ")\n",
    "scheduler = getattr(torch.optim.lr_scheduler, model_parameters[\"scheduler\"])(\n",
    "    optimizer, **model_parameters[\"scheduler_args\"]\n",
    ")\n",
    "\n",
    "##Start training\n",
    "model_errors = training.trainer(\n",
    "    rank,\n",
    "    world_size,\n",
    "    model_errors,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    training_parameters[\"loss\"],\n",
    "    train_loader_e,\n",
    "    val_loader_e,\n",
    "    train_sampler,\n",
    "    model_parameters[\"epochs\"],\n",
    "    training_parameters[\"verbosity\"],\n",
    "    \"my_model_error_temp.pth\",\n",
    ")\n",
    "\n",
    "train_error_e, train_out_e = training.evaluate(\n",
    "    train_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "val_error_e, val_out_e = training.evaluate(\n",
    "    val_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "test_error_e, test_out_e = training.evaluate(\n",
    "    test_loader_e, model_errors, training_parameters[\"loss\"], rank, out=True)\n",
    "\n",
    "\n",
    "target_train_e = pd.DataFrame(train_out_e, columns=['index', 'target_error', 'predicted_error'])\n",
    "target_val_e = pd.DataFrame(val_out_e, columns=['index', 'target_error', 'predicted_error']) \n",
    "target_test_e = pd.DataFrame(test_out_e, columns=['index', 'target_error', 'predicted_error']) \n",
    "target_errors_e = pd.concat([target_train_e,target_val_e,target_test_e], axis = 0)\n",
    "target_errors_e = target_errors_e.sort_values(list(target_errors_e), ascending=True)\n",
    "target_errors_e['error_2'] = np.absolute(target_errors_e['target_error'].apply(float) - target_errors_e['predicted_error'].apply(float))\n",
    "\n",
    "target_val_e_2 = copy.copy(target_val_e)\n",
    "target_val_e_2['target_error'] = target_val_e_2['target_error'].apply(float)\n",
    "target_val_e_2['predicted_error'] = target_val_e_2['predicted_error'].apply(float)\n",
    "target_val_e_2['alpha'] = np.abs(target_val_e_2['target_error']-target_val_e_2['predicted_error'])\n",
    "\n",
    "target_val_e_2 = target_val_e_2.sort_values(['alpha'], axis=0, ascending=True)\n",
    "alpha = np.percentile(target_val_e_2['alpha'], 95)\n",
    "\n",
    "target_train_e['predicted_error'] = target_train_e['predicted_error'].apply(float)\n",
    "target_train_e['lower_error_confidence_level'] = target_train_e['predicted_error'] - alpha\n",
    "target_train_e['upper_error_confidence_level'] = target_train_e['predicted_error'] + alpha\n",
    "\n",
    "target_val_e['predicted_error'] = target_val_e['predicted_error'].apply(float)\n",
    "target_val_e['lower_error_confidence_level'] = target_val_e['predicted_error'] - alpha\n",
    "target_val_e['upper_error_confidence_level'] = target_val_e['predicted_error'] + alpha\n",
    "\n",
    "target_test_e['predicted_error'] = target_test_e['predicted_error'].apply(float)\n",
    "target_test_e['lower_error_confidence_level'] = target_test_e['predicted_error'] - alpha\n",
    "target_test_e['upper_error_confidence_level'] = target_test_e['predicted_error'] + alpha\n",
    "\n",
    "\n",
    "target_train_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_train.csv'), index = False, header=False)\n",
    "target_val_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_val.csv'), index = False, header=False)\n",
    "target_test_e.to_csv(os.path.join(os.getcwd(),'error_prediction_conf_test.csv'), index = False, header=False)\n",
    "\n",
    "        ##Write outputs\n",
    "if job_parameters[\"write_output\"] == \"True\":\n",
    "\n",
    "    training.write_results(\n",
    "        train_out, str(job_parameters[\"job_name\"]) + \"_train_outputs.csv\"\n",
    "    )\n",
    "    if val_loader != None:\n",
    "        training.write_results(\n",
    "            val_out, str(job_parameters[\"job_name\"]) + \"_val_outputs.csv\"\n",
    "        )\n",
    "    if test_loader != None:\n",
    "        training.write_results(\n",
    "            test_out, str(job_parameters[\"job_name\"]) + \"_test_outputs.csv\"\n",
    "        )\n",
    "\n",
    "if rank not in (\"cpu\", \"cuda\"):\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "##Write out model performance to file\n",
    "error_values = np.array((train_error.cpu(), val_error.cpu(), test_error.cpu()))\n",
    "if job_parameters.get(\"write_error\") == \"True\":\n",
    "    np.savetxt(\n",
    "        job_parameters[\"job_name\"] + \"_errorvalues.csv\",\n",
    "        error_values[np.newaxis, ...],\n",
    "        delimiter=\",\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ba0a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6a3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "old_Cuda",
   "language": "python",
   "name": "old_cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
